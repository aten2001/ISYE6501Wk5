---
title: "ISYE6501 HW5"
author: "Keh-Harng Feng"
date: "June 16, 2017"
header-includes:
    - \usepackage{placeins}
output: 
  bookdown::html_document2:
    fig_caption: TRUE
    toc: FALSE
urlcolor: blue
---
```{r setup, include=FALSE}
library('knitr')
library('caret')
library('glmnet')
library('glmnetUtils')
library('parallel')
library('FrF2')

opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, tidy = TRUE, cache = TRUE)
options(digits = 4)
```

## Preface
This is a reproducible report with most of the codes doing the heavy lifting hidden in the background. You can download the source code of the report by [clicking here](https://github.com/fengkehh/ISYE6501Wk5/blob/master/HW5_Report.Rmd). All of the information you need as a reviewer is in the report, not in the code. You should NEVER run any R scripts from an untrusted source on your own computer.  

# Question 1
**Using the crime data set from Homework 3, build a regression model using:**
1. **Stepwise regression**
2. **Lasso**
3. **Elastic net**
**For Parts 2 and 3, remember to scale the data first – otherwise, the regression coefficients will be on different scales and the constraint won’t have the desired effect.**
**For Parts 2 and 3, use the glmnet function in R.**

```{r}
q1_data <- read.table('uscrime.txt', header = TRUE)

q1_data$So <- factor(q1_data$So)

n <- nrow(q1_data)

set.seed(123)
inTrain <- sample(1:n, size = ceiling(n*0.9))

q1.train <- q1_data[inTrain,]
q1.test <- q1_data[-inTrain,]

# Hack
q1.test <- rbind(q1.train[1,], q1.test)
q1.test <- q1.test[-1,]

# Preprocessing
preProc <- preProcess(q1.train[,-16], method = c('BoxCox', 'center', 'scale'))
q1.train.preproc <- predict(preProc, q1.train)
```

Variable `So` is converted to a 2-level factor as it is categorical. The data is then splitted into a training (~90%) and testing (~10%) sets as usual. Predictor variables from the training set are transformed using Box-Cox transformation then centered and scaled (see my [HW3 Report](https://github.com/fengkehh/ISYE6501WK3/blob/master/HW3_Report.pdf)). The response `Crime` is not preprocessed.

## Stepwise Regression
Stepwise regression in both directions is carried out using the `step()` function and AIC as the selection metric. The initial starting point is an OLS model using all features. 

```{r, results = 'hide'}
# Initial Model for Stepwise Selection
model.init <- lm(Crime ~ ., data = q1.train.preproc)

# Stepwise Selection
model.stepboth <- step(model.init, direction = 'both')
```

The final model selected is shown below:
```{r}
summary(model.stepboth)
```

While the process cut down the number of predictors from 15 to 10, features `NW`, `U1`, and `Wealth` show p-values higher than 0.05. It should be noted that model statistics from stepwise regression often [do not mean what they are supposed to mean](https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856). It is generally frowned upon as a naive method for feature selection.

## LASSO Regression
A LASSO regression is fitted using `glmnet` ($\alpha = 0$). 10-fold cross-validation is used to generate estimated MSE to select the optimal regularization parameter $\lambda$. The resulting LASSO model contains the following coefficients:

```{r}
model.lasso <- cv.glmnet(Crime ~., data = q1.train.preproc, 
                      family = 'gaussian', alpha = 1, type.measure = 'mse', 
                      keep = TRUE, nfolds = 10)

show_coeffs <- function(model) {
    coeffs <- coef(model, s = "lambda.min")    
    df <- data.frame(name = dimnames(coeffs)[[1]][coeffs@i+1], 
                     coefficient = coeffs@x)
    return(df)
}

kable(show_coeffs(model.lasso))
```

9 features out of 15 are selected for the LASSO model. Notice that from HW3 we know `Po1` and `Po2` are highly correlated. In fact, Po1 was removed during exploratory analysis due to high colinearity check in my [HW3 Report](https://github.com/fengkehh/ISYE6501WK3/blob/master/HW3_Report.pdf). Here LASSO does not remove either predictor. Instead `Po1` is assigned a much heavier influence compared to `Po2`. This is one of the common criticisms about LASSO: one of the highly correlated variables tends to be selected and assigned heavy influence while others are penalized seemingly randomly.

## Elastic Net
Before we move on to elastic net, Figure \@ref(fig:alphaplots) shows the effect of $\alpha$ on model MSE (all CV folds are fixed to the folds used to build the LASSO model).

```{r alphaplots, fig.cap = '10-CV estimated model MSE vs log(lambda).'}
cv0 <- cv.glmnet(Crime ~., data = q1.train.preproc, alpha = 0, foldid = model.lasso$foldid)
cv05 <- cv.glmnet(Crime ~., data = q1.train.preproc, alpha = 0.5, foldid = model.lasso$foldid)
cv1 <- cv.glmnet(Crime ~., data = q1.train.preproc, alpha = 1, foldid = model.lasso$foldid)

log_lambda_str <- expression('log('~lambda~')')
plot(log(cv1$lambda),cv1$cvm,pch=19,col="red",xlab=log_lambda_str,ylab=cv1$name)
points(log(cv05$lambda),cv05$cvm,pch=19,col="grey")
points(log(cv0$lambda),cv0$cvm,pch=19,col="blue")
legend("topleft",legend=c("alpha= 1","alpha= .5","alpha 0"),pch=19,col=c("red","grey","blue"))
```

\FloatBarrier

It seems that $\alpha = 0.5$ achieves the smallest MSE compared to both LASSO ($\alpha = 1$) and ridge regression ($\alpha = 0$). A grid-search is done to find the alpha value that corresponds to the minimum estimated MSE using the same CV folds. 

```{r}
# Grid search
q1_grid_search <- function(alpha_range = c(0, 1), grid_n = 1001, foldid) {

    alphas <- seq(from = alpha_range[1], to = alpha_range[2],
                  length.out = grid_n)

    mse_vals <- rep(0, grid_n)

    q1_helper <- function(alpha, data, foldid) {

        test_model <- glmnetUtils::cv.glmnet(Crime ~., data = data,
                                             alpha = alpha, foldid = foldid)
        
        min_mse <- min(test_model$cvm)

        return(min_mse)
    }

    cl <- makePSOCKcluster(detectCores(logical = FALSE))
    clusterExport(cl = cl, varlist = list('alphas', 'q1.train.preproc'),
                  envir = environment())

    mse_vals <- parSapply(cl = cl, X = alphas, FUN = q1_helper, 
                          data = q1.train.preproc, foldid = foldid)

    stopCluster(cl)

    ans = list(min_ind = min(mse_vals), alpha = alphas[which.min(mse_vals)], 
               mses = mse_vals)
    
    return(ans)
}

alpha_search <- q1_grid_search(foldid = model.lasso$foldid)
model.elastic <- cv.glmnet(Crime ~., data = q1.train.preproc, 
                           alpha = alpha_search$alpha, foldid = model.lasso$foldid)

```

The minimum MSE found vs corresponding alpha values is shown in Figure \@ref(fig:msealphas). With this in mind, an elastic net model is constructed using the alpha value that corresponds to the smallest MSE, $\alpha =$ `r alpha_search$alpha`. 

```{r msealphas, fig.cap = 'Min MSE vs alpha'}
alphas <- seq(from = 0, to = 1, length.out = 1001)
plot(alphas, alpha_search$mses, ylab = '10-CV Estimated MSE', xlab = expression(alpha~' (LASSO/ridge weighting)'))
```

\FloatBarrier

The model coefficients are shown below. 
```{r}
model.elastic <- cv.glmnet(Crime ~., data = q1.train.preproc, 
                           alpha = alpha_search$alpha, foldid = model.lasso$foldid)
kable(show_coeffs(model.elastic))

mse.lasso <- min(model.lasso$cvm)
mse.elastic <- min(model.elastic$cvm)
```

With 11 out of 15 features selected the model is perhaps in danger of overfitting. However, in contrast to LASSO the colinear predictors `Po1` and `Po2` are assigned roughly equal weights. This is an expected behavior of elastic net regression. The estimated MSE as a function of log(lambda) from both LASSO and elastic net is shown in Figure \@ref(fig:glmnetcompare). The minimum MSE achieved by LASSO is $`r mse.lasso`$ while that of the elastic net is $`r mse.elastic`$. 

```{r glmnetcompare, fig.cap = '10-CV estimated MSE vs log(lambda) between LASSO and elastic net (alpha ~0.202).'}
plot(log(model.elastic$lambda), model.elastic$cvm, xlab = log_lambda_str, ylab = 'Estimated MSE')
points(log(model.lasso$lambda), model.lasso$cvm, col = 'red')
legend('topright', pch = 'o', legend = c('Elastic Net', 'LASSO'), col = c('black', 'red'))
```

## Model Performance

Model performance is evaluated using the test set with the RMSE as the metric. The result is shown below.

```{r}
q1.test.preproc <- predict(preProc, q1.test)

pred.stepboth <- predict(model.stepboth, q1.test.preproc)
pred.lasso <- predict(model.lasso, q1.test.preproc)
pred.elastic <- predict(model.elastic, q1.test.preproc)

rmse.stepboth <- sqrt(ModelMetrics::mse(q1.test.preproc$Crime, pred.stepboth))
rmse.lasso <- sqrt(ModelMetrics::mse(q1.test.preproc$Crime, pred.lasso))
rmse.elastic <- sqrt(ModelMetrics::mse(q1.test.preproc$Crime, pred.elastic))

rmse <- data.frame(Model = c('Stepwise', 'LASSO', 'Elastic Net'), RMSE = c(rmse.stepboth, rmse.lasso, rmse.elastic))

kable(rmse)
```

Surprisingly stepwise regression is able to achieve the best test set RMSE by far. Both LASSO and elastic net are roughly equal and significantly worse than stepwise regression. I do not have any satisfactory explanation for this besides that the sample size is very small to begin with and that the result could easily be due to any one of the model hitting a fortunate (or unfotunate) jackpot.

\pagebreak

# Question 2
**Describe a situation or problem from your job, everyday life, current events, etc., for which a design of experiments approach would be appropriate.**

I was the acting manager and coach of an amateur softball team a couple of years ago. One of my responsibilities was to oversee the design, manufacturing and distribution of the team jerseys. With over 20 players of vastly different backgrounds and ages it was not easy to find an optimal design that satisfied everyone. People have different preferences when it came to material, color, font, and graphics. If I am given the same task again, once the team administrators and designers figure out a limited set of choices for the different elements on the jersey full/fractional factorial design can be utilized to generate different virtual samples using computer graphics. These can then be used in a survey for the team members in order to reach a consensus for an optimal choice.

# Question 3
**To determine the value of 10 different yes/no features to the market value of a house (large yard, solar roof, etc.), a real estate agent plans to survey 50 potential buyers, showing a fictitious house with different combinations of features. To reduce the survey size, the agent wants to show just 16 fictitious houses. Use R’s FrF2 function (in the FrF2 package) to find a fractional factorial design for this experiment: what set of features should each of the 16 fictitious houses? Note: the output of FrF2 is “1” (include) or “-1” (don’t include) for each feature.**

The 50 buyers are assumed to hold different preferences and weightings regarding different features (ie: the same design may elicit different reponses to different buyers). Therefore it is important that each buyer browses all 16 designs. The order of browsing is assumed to be inconsequential. This can be described as designing an experiment with 10 two-level features that will be run 16 times, each time involving a survey of 50 people and each run with a different combination of feature levels. Since all buyers will browse all 16 designs and order doesn't matter the number of buyers is actually not important. The design matrix generated by `FrF2()` is shown below:

```{r}
feature_names <- c('Feature 1','Feature 2','Feature 3','Feature 4','Feature 5', 
                   'Feature 6','Feature 7','Feature 8','Feature 9','Feature 10')

design <- FrF2(nruns = 16, factor.names = feature_names)

design_frame <- data.frame(design)

for (varname in names(design_frame)) {
    design_frame[,varname] <- factor(design_frame[,varname], levels = c(-1, 1), 
                                   labels = c('No', 'Yes'))
}

kable(design_frame)
```

# Question 4
**For each of the following distributions, give an example of data that you would expect to follow this distribution (besides the examples already discussed in class).**

As a someone with a MS in physics I am going to troll a bit and put on my *Big Bang Theory* hat:

a. **Binomial**
The number of female partners I got randomly assigned to during my third year undergraduate physics lab courses. FYI, the expected value as I recall was just about zero (fine let's call it 0.5). Each class has a total of 10 lab sessions.

b. **Geometric**
The probability distribution of the number of times I need to roll a d20 dice before landing a critical hit during a game of Dungeon's & Dragons. You land a critical hit when you roll a 20 (usually).

c. **Poisson**
The number of times some engineers got snubbed asking physicists questions in the last week. Don't worry, mathematicians tend to turn the table on us.

d. **Exponential**
The amount of time that passes before another engineer gets snubbed by a physicist. On behalf of my fellow physicists, I apologize.

e. **Weibull**
The probability of people switching out of undergraduate physics as a function of their year of study. Believe me, by the time you get to the higher years those who stay either know exactly what they are doing, don't mind double expressos at 1AM or a bit of both.
